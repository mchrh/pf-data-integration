2024-11-22 12:09:49,282 - DataLoader - INFO - Initialized Spark session
2024-11-22 12:09:49,282 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-22 12:09:50,356 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-22 12:09:51,830 - DataLoader - ERROR - Error processing /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx: An error occurred while calling o50.parquet.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (192.168.1.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
    raise PySparkRuntimeError(
    ...<5 lines>...
    )
pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 13) than that in driver 3.10, PySpark cannot run with different minor versions.
Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1570)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1570)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
    raise PySparkRuntimeError(
    ...<5 lines>...
    )
pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 13) than that in driver 3.10, PySpark cannot run with different minor versions.
Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2024-11-22 12:09:51,830 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-22 12:09:51,923 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-22 12:09:52,225 - DataLoader - ERROR - Error processing /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx: An error occurred while calling o106.parquet.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 1.0 failed 1 times, most recent failure: Lost task 6.0 in stage 1.0 (TID 16) (192.168.1.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
    raise PySparkRuntimeError(
    ...<5 lines>...
    )
pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 13) than that in driver 3.10, PySpark cannot run with different minor versions.
Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1570)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1570)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
    raise PySparkRuntimeError(
    ...<5 lines>...
    )
pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 13) than that in driver 3.10, PySpark cannot run with different minor versions.
Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2024-11-22 12:09:52,226 - DataLoader - INFO - Successfully loaded all static data
2024-11-22 12:09:52,340 - DataLoader - ERROR - Error verifying data: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.
2024-11-22 12:09:52,341 - py4j.clientserver - INFO - Closing down clientserver connection
2024-11-22 12:11:47,900 - DataLoader - INFO - Initialized Spark session
2024-11-22 12:11:47,900 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-22 12:11:47,984 - DataLoader - INFO - Inferred schema: StructType([StructField('PROGRAM_ID', StringType(), True), StructField('PARAMETER', StringType(), True), StructField('START_YEAR', LongType(), True), StructField('END_YEAR', DoubleType(), True), StructField('METHOD', StringType(), True), StructField('METHOD_DESCRIPTION', StringType(), True)])
2024-11-22 12:11:48,845 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-22 12:11:50,141 - DataLoader - ERROR - Error processing /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx: An error occurred while calling o52.parquet.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (192.168.1.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
    raise PySparkRuntimeError(
    ...<5 lines>...
    )
pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 13) than that in driver 3.10, PySpark cannot run with different minor versions.
Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1570)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1570)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
    raise PySparkRuntimeError(
    ...<5 lines>...
    )
pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 13) than that in driver 3.10, PySpark cannot run with different minor versions.
Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2024-11-22 12:11:50,142 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-22 12:11:50,189 - DataLoader - INFO - Inferred schema: StructType([StructField('SITE_ID', StringType(), True), StructField('PROGRAM_ID', StringType(), True), StructField('LATDD', DoubleType(), True), StructField('LONDD', DoubleType(), True), StructField('LATDD_CENTROID', DoubleType(), True), StructField('LONDD_CENTROID', DoubleType(), True), StructField('SITE_NAME', StringType(), True), StructField('COMID', DoubleType(), True), StructField('FEATUREID', LongType(), True), StructField('COUNTY', StringType(), True), StructField('STATE', StringType(), True), StructField('ECOREGION_I', LongType(), True), StructField('ECOREGION_II', DoubleType(), True), StructField('ECOREGION_III', StringType(), True), StructField('ECOREGION_IV', StringType(), True), StructField('ECONAME_I', StringType(), True), StructField('ECONAME_II', StringType(), True), StructField('ECONAME_III', StringType(), True), StructField('ECONAME_IV', StringType(), True), StructField('LAKE_DEPTH_MAX', DoubleType(), True), StructField('LAKE_DEPTH_MEAN', DoubleType(), True), StructField('LAKE_RET', DoubleType(), True), StructField('LAKE_AREA_HA', DoubleType(), True), StructField('LAKE_AREA_NHD2', DoubleType(), True), StructField('WSHD_AREA_HA', DoubleType(), True), StructField('SITE_ELEV', DoubleType(), True), StructField('WSHD_ELEV_AVG', DoubleType(), True), StructField('WSHD_ELEV_MIN', DoubleType(), True), StructField('WSHD_ELEV_MAX', DoubleType(), True), StructField('WSHD_SLOPE_AVG', DoubleType(), True), StructField('UPDATE_DATE', TimestampType(), True)])
2024-11-22 12:11:50,249 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-22 12:11:50,436 - DataLoader - ERROR - Error processing /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx: An error occurred while calling o104.parquet.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 7) (192.168.1.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
    raise PySparkRuntimeError(
    ...<5 lines>...
    )
pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 13) than that in driver 3.10, PySpark cannot run with different minor versions.
Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1570)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1570)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
    raise PySparkRuntimeError(
    ...<5 lines>...
    )
pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 13) than that in driver 3.10, PySpark cannot run with different minor versions.
Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2024-11-22 12:11:50,437 - DataLoader - ERROR - Failed to load some data files
2024-11-22 12:11:50,437 - DataLoader - ERROR - Failed to load data
2024-11-22 12:11:50,437 - py4j.clientserver - INFO - Closing down clientserver connection
2024-11-22 12:13:00,856 - DataLoader - INFO - Initialized Spark session
2024-11-22 12:13:00,857 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-22 12:13:00,962 - DataLoader - INFO - Inferred schema: StructType([StructField('PROGRAM_ID', StringType(), True), StructField('PARAMETER', StringType(), True), StructField('START_YEAR', LongType(), True), StructField('END_YEAR', DoubleType(), True), StructField('METHOD', StringType(), True), StructField('METHOD_DESCRIPTION', StringType(), True)])
2024-11-22 12:13:01,897 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-22 12:13:03,837 - DataLoader - INFO - Successfully wrote 362 records to /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-22 12:13:03,837 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-22 12:13:03,885 - DataLoader - INFO - Inferred schema: StructType([StructField('SITE_ID', StringType(), True), StructField('PROGRAM_ID', StringType(), True), StructField('LATDD', DoubleType(), True), StructField('LONDD', DoubleType(), True), StructField('LATDD_CENTROID', DoubleType(), True), StructField('LONDD_CENTROID', DoubleType(), True), StructField('SITE_NAME', StringType(), True), StructField('COMID', DoubleType(), True), StructField('FEATUREID', LongType(), True), StructField('COUNTY', StringType(), True), StructField('STATE', StringType(), True), StructField('ECOREGION_I', LongType(), True), StructField('ECOREGION_II', DoubleType(), True), StructField('ECOREGION_III', StringType(), True), StructField('ECOREGION_IV', StringType(), True), StructField('ECONAME_I', StringType(), True), StructField('ECONAME_II', StringType(), True), StructField('ECONAME_III', StringType(), True), StructField('ECONAME_IV', StringType(), True), StructField('LAKE_DEPTH_MAX', DoubleType(), True), StructField('LAKE_DEPTH_MEAN', DoubleType(), True), StructField('LAKE_RET', DoubleType(), True), StructField('LAKE_AREA_HA', DoubleType(), True), StructField('LAKE_AREA_NHD2', DoubleType(), True), StructField('WSHD_AREA_HA', DoubleType(), True), StructField('SITE_ELEV', DoubleType(), True), StructField('WSHD_ELEV_AVG', DoubleType(), True), StructField('WSHD_ELEV_MIN', DoubleType(), True), StructField('WSHD_ELEV_MAX', DoubleType(), True), StructField('WSHD_SLOPE_AVG', DoubleType(), True), StructField('UPDATE_DATE', TimestampType(), True)])
2024-11-22 12:13:03,928 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-22 12:13:04,269 - DataLoader - INFO - Successfully wrote 175 records to /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-22 12:13:04,269 - DataLoader - INFO - Successfully loaded all static data
2024-11-22 12:13:04,436 - DataLoader - INFO - === Data Verification ===
2024-11-22 12:13:04,583 - DataLoader - INFO - Methods data count: 362
2024-11-22 12:13:04,584 - DataLoader - INFO - Methods columns: PROGRAM_ID, PARAMETER, START_YEAR, END_YEAR, METHOD, METHOD_DESCRIPTION
2024-11-22 12:13:04,641 - DataLoader - INFO - Site Information data count: 175
2024-11-22 12:13:04,642 - DataLoader - INFO - Site Information columns: SITE_ID, PROGRAM_ID, LATDD, LONDD, LATDD_CENTROID, LONDD_CENTROID, SITE_NAME, COMID, FEATUREID, COUNTY, STATE, ECOREGION_I, ECOREGION_II, ECOREGION_III, ECOREGION_IV, ECONAME_I, ECONAME_II, ECONAME_III, ECONAME_IV, LAKE_DEPTH_MAX, LAKE_DEPTH_MEAN, LAKE_RET, LAKE_AREA_HA, LAKE_AREA_NHD2, WSHD_AREA_HA, SITE_ELEV, WSHD_ELEV_AVG, WSHD_ELEV_MIN, WSHD_ELEV_MAX, WSHD_SLOPE_AVG, UPDATE_DATE
2024-11-22 12:13:04,642 - DataLoader - INFO - 
Sample of Methods data:
2024-11-22 12:13:05,173 - DataLoader - INFO - 
Sample of Site Information data:
2024-11-22 12:13:05,303 - py4j.clientserver - INFO - Closing down clientserver connection
2024-11-28 08:58:32,374 - DataLoader - INFO - Initialized Spark session
2024-11-28 08:58:32,374 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-28 08:58:32,794 - DataLoader - INFO - Inferred schema: StructType([StructField('PROGRAM_ID', StringType(), True), StructField('PARAMETER', StringType(), True), StructField('START_YEAR', LongType(), True), StructField('END_YEAR', DoubleType(), True), StructField('METHOD', StringType(), True), StructField('METHOD_DESCRIPTION', StringType(), True)])
2024-11-28 08:58:33,760 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 08:58:35,837 - DataLoader - INFO - Successfully wrote 362 records to /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 08:58:35,837 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-28 08:58:35,891 - DataLoader - INFO - Inferred schema: StructType([StructField('SITE_ID', StringType(), True), StructField('PROGRAM_ID', StringType(), True), StructField('LATDD', DoubleType(), True), StructField('LONDD', DoubleType(), True), StructField('LATDD_CENTROID', DoubleType(), True), StructField('LONDD_CENTROID', DoubleType(), True), StructField('SITE_NAME', StringType(), True), StructField('COMID', DoubleType(), True), StructField('FEATUREID', LongType(), True), StructField('COUNTY', StringType(), True), StructField('STATE', StringType(), True), StructField('ECOREGION_I', LongType(), True), StructField('ECOREGION_II', DoubleType(), True), StructField('ECOREGION_III', StringType(), True), StructField('ECOREGION_IV', StringType(), True), StructField('ECONAME_I', StringType(), True), StructField('ECONAME_II', StringType(), True), StructField('ECONAME_III', StringType(), True), StructField('ECONAME_IV', StringType(), True), StructField('LAKE_DEPTH_MAX', DoubleType(), True), StructField('LAKE_DEPTH_MEAN', DoubleType(), True), StructField('LAKE_RET', DoubleType(), True), StructField('LAKE_AREA_HA', DoubleType(), True), StructField('LAKE_AREA_NHD2', DoubleType(), True), StructField('WSHD_AREA_HA', DoubleType(), True), StructField('SITE_ELEV', DoubleType(), True), StructField('WSHD_ELEV_AVG', DoubleType(), True), StructField('WSHD_ELEV_MIN', DoubleType(), True), StructField('WSHD_ELEV_MAX', DoubleType(), True), StructField('WSHD_SLOPE_AVG', DoubleType(), True), StructField('UPDATE_DATE', TimestampType(), True)])
2024-11-28 08:58:35,955 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-28 08:58:36,259 - DataLoader - INFO - Successfully wrote 175 records to /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-28 08:58:36,259 - DataLoader - INFO - Successfully loaded all static data
2024-11-28 08:58:36,403 - DataLoader - INFO - === Data Verification ===
2024-11-28 08:58:36,535 - DataLoader - INFO - Methods data count: 362
2024-11-28 08:58:36,536 - DataLoader - INFO - Methods columns: PROGRAM_ID, PARAMETER, START_YEAR, END_YEAR, METHOD, METHOD_DESCRIPTION
2024-11-28 08:58:36,609 - DataLoader - INFO - Site Information data count: 175
2024-11-28 08:58:36,610 - DataLoader - INFO - Site Information columns: SITE_ID, PROGRAM_ID, LATDD, LONDD, LATDD_CENTROID, LONDD_CENTROID, SITE_NAME, COMID, FEATUREID, COUNTY, STATE, ECOREGION_I, ECOREGION_II, ECOREGION_III, ECOREGION_IV, ECONAME_I, ECONAME_II, ECONAME_III, ECONAME_IV, LAKE_DEPTH_MAX, LAKE_DEPTH_MEAN, LAKE_RET, LAKE_AREA_HA, LAKE_AREA_NHD2, WSHD_AREA_HA, SITE_ELEV, WSHD_ELEV_AVG, WSHD_ELEV_MIN, WSHD_ELEV_MAX, WSHD_SLOPE_AVG, UPDATE_DATE
2024-11-28 08:58:36,610 - DataLoader - INFO - 
Sample of Methods data:
2024-11-28 08:58:37,164 - DataLoader - INFO - 
Sample of Site Information data:
2024-11-28 08:58:37,297 - py4j.clientserver - INFO - Closing down clientserver connection
2024-11-28 09:19:04,761 - DataLoader - INFO - Initialized Spark session
2024-11-28 09:19:04,762 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-28 09:19:04,830 - DataLoader - INFO - Inferred schema: StructType([StructField('PROGRAM_ID', StringType(), True), StructField('PARAMETER', StringType(), True), StructField('START_YEAR', LongType(), True), StructField('END_YEAR', DoubleType(), True), StructField('METHOD', StringType(), True), StructField('METHOD_DESCRIPTION', StringType(), True)])
2024-11-28 09:19:05,670 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 09:19:07,644 - DataLoader - INFO - Successfully wrote 362 records to /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 09:19:07,645 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-28 09:19:07,698 - DataLoader - INFO - Inferred schema: StructType([StructField('SITE_ID', StringType(), True), StructField('PROGRAM_ID', StringType(), True), StructField('LATDD', DoubleType(), True), StructField('LONDD', DoubleType(), True), StructField('LATDD_CENTROID', DoubleType(), True), StructField('LONDD_CENTROID', DoubleType(), True), StructField('SITE_NAME', StringType(), True), StructField('COMID', DoubleType(), True), StructField('FEATUREID', LongType(), True), StructField('COUNTY', StringType(), True), StructField('STATE', StringType(), True), StructField('ECOREGION_I', LongType(), True), StructField('ECOREGION_II', DoubleType(), True), StructField('ECOREGION_III', StringType(), True), StructField('ECOREGION_IV', StringType(), True), StructField('ECONAME_I', StringType(), True), StructField('ECONAME_II', StringType(), True), StructField('ECONAME_III', StringType(), True), StructField('ECONAME_IV', StringType(), True), StructField('LAKE_DEPTH_MAX', DoubleType(), True), StructField('LAKE_DEPTH_MEAN', DoubleType(), True), StructField('LAKE_RET', DoubleType(), True), StructField('LAKE_AREA_HA', DoubleType(), True), StructField('LAKE_AREA_NHD2', DoubleType(), True), StructField('WSHD_AREA_HA', DoubleType(), True), StructField('SITE_ELEV', DoubleType(), True), StructField('WSHD_ELEV_AVG', DoubleType(), True), StructField('WSHD_ELEV_MIN', DoubleType(), True), StructField('WSHD_ELEV_MAX', DoubleType(), True), StructField('WSHD_SLOPE_AVG', DoubleType(), True), StructField('UPDATE_DATE', TimestampType(), True)])
2024-11-28 09:19:07,758 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-28 09:19:08,044 - DataLoader - INFO - Successfully wrote 175 records to /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-28 09:19:08,044 - DataLoader - INFO - Successfully loaded all static data
2024-11-28 09:19:08,193 - DataLoader - INFO - === Data Verification ===
2024-11-28 09:19:08,339 - DataLoader - INFO - Methods data count: 362
2024-11-28 09:19:08,340 - DataLoader - INFO - Methods columns: PROGRAM_ID, PARAMETER, START_YEAR, END_YEAR, METHOD, METHOD_DESCRIPTION
2024-11-28 09:19:08,401 - DataLoader - INFO - Site Information data count: 175
2024-11-28 09:19:08,402 - DataLoader - INFO - Site Information columns: SITE_ID, PROGRAM_ID, LATDD, LONDD, LATDD_CENTROID, LONDD_CENTROID, SITE_NAME, COMID, FEATUREID, COUNTY, STATE, ECOREGION_I, ECOREGION_II, ECOREGION_III, ECOREGION_IV, ECONAME_I, ECONAME_II, ECONAME_III, ECONAME_IV, LAKE_DEPTH_MAX, LAKE_DEPTH_MEAN, LAKE_RET, LAKE_AREA_HA, LAKE_AREA_NHD2, WSHD_AREA_HA, SITE_ELEV, WSHD_ELEV_AVG, WSHD_ELEV_MIN, WSHD_ELEV_MAX, WSHD_SLOPE_AVG, UPDATE_DATE
2024-11-28 09:19:08,402 - DataLoader - INFO - 
Sample of Methods data:
2024-11-28 09:19:08,990 - DataLoader - INFO - 
Sample of Site Information data:
2024-11-28 09:19:09,135 - py4j.clientserver - INFO - Closing down clientserver connection
2024-11-28 09:25:59,826 - DataLoader - INFO - Initialized Spark session
2024-11-28 09:25:59,826 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-28 09:25:59,928 - DataLoader - INFO - Inferred schema: StructType([StructField('PROGRAM_ID', StringType(), True), StructField('PARAMETER', StringType(), True), StructField('START_YEAR', LongType(), True), StructField('END_YEAR', DoubleType(), True), StructField('METHOD', StringType(), True), StructField('METHOD_DESCRIPTION', StringType(), True)])
2024-11-28 09:26:00,832 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 09:26:02,788 - DataLoader - INFO - Successfully wrote 362 records to /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 09:26:02,788 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-28 09:26:02,822 - DataLoader - INFO - Inferred schema: StructType([StructField('PROGRAM_ID', StringType(), True), StructField('PARAMETER', StringType(), True), StructField('START_YEAR', LongType(), True), StructField('END_YEAR', DoubleType(), True), StructField('METHOD', StringType(), True), StructField('METHOD_DESCRIPTION', StringType(), True)])
2024-11-28 09:26:02,841 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/raw
2024-11-28 09:26:03,009 - DataLoader - INFO - Successfully wrote 362 records to /Users/work/Documents/pf-data-integration/data/raw
2024-11-28 09:26:03,009 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-28 09:26:03,009 - DataLoader - ERROR - Error processing /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx: [Errno 2] No such file or directory: '/Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx'
2024-11-28 09:26:03,009 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-28 09:26:03,010 - DataLoader - ERROR - Error processing /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx: [Errno 2] No such file or directory: '/Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx'
2024-11-28 09:26:03,010 - DataLoader - ERROR - Failed to load some data files
2024-11-28 09:26:03,010 - DataLoader - ERROR - Failed to load data
2024-11-28 09:26:03,010 - py4j.clientserver - INFO - Closing down clientserver connection
2024-11-28 09:27:39,328 - DataLoader - INFO - Initialized Spark session
2024-11-28 09:27:39,329 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-28 09:27:39,456 - DataLoader - INFO - Inferred schema: StructType([StructField('PROGRAM_ID', StringType(), True), StructField('PARAMETER', StringType(), True), StructField('START_YEAR', LongType(), True), StructField('END_YEAR', DoubleType(), True), StructField('METHOD', StringType(), True), StructField('METHOD_DESCRIPTION', StringType(), True)])
2024-11-28 09:27:40,455 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 09:27:42,436 - DataLoader - INFO - Successfully wrote 362 records to /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 09:27:42,436 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-28 09:27:42,470 - DataLoader - INFO - Inferred schema: StructType([StructField('PROGRAM_ID', StringType(), True), StructField('PARAMETER', StringType(), True), StructField('START_YEAR', LongType(), True), StructField('END_YEAR', DoubleType(), True), StructField('METHOD', StringType(), True), StructField('METHOD_DESCRIPTION', StringType(), True)])
2024-11-28 09:27:42,490 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/raw
2024-11-28 09:27:42,659 - DataLoader - INFO - Successfully wrote 362 records to /Users/work/Documents/pf-data-integration/data/raw
2024-11-28 09:27:42,659 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-28 09:27:42,659 - DataLoader - ERROR - Error processing /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx: [Errno 2] No such file or directory: '/Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx'
2024-11-28 09:27:42,659 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-28 09:27:42,659 - DataLoader - ERROR - Error processing /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx: [Errno 2] No such file or directory: '/Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx'
2024-11-28 09:27:42,660 - DataLoader - ERROR - Failed to load some data files
2024-11-28 09:27:42,660 - DataLoader - ERROR - Failed to load data
2024-11-28 09:27:42,661 - py4j.clientserver - INFO - Closing down clientserver connection
2024-11-28 09:28:38,800 - DataLoader - INFO - Initialized Spark session
2024-11-28 09:28:38,800 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-28 09:28:38,920 - DataLoader - INFO - Inferred schema: StructType([StructField('PROGRAM_ID', StringType(), True), StructField('PARAMETER', StringType(), True), StructField('START_YEAR', LongType(), True), StructField('END_YEAR', DoubleType(), True), StructField('METHOD', StringType(), True), StructField('METHOD_DESCRIPTION', StringType(), True)])
2024-11-28 09:28:39,892 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 09:28:41,880 - DataLoader - INFO - Successfully wrote 362 records to /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 09:28:41,880 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-28 09:28:41,932 - DataLoader - INFO - Inferred schema: StructType([StructField('SITE_ID', StringType(), True), StructField('PROGRAM_ID', StringType(), True), StructField('LATDD', DoubleType(), True), StructField('LONDD', DoubleType(), True), StructField('LATDD_CENTROID', DoubleType(), True), StructField('LONDD_CENTROID', DoubleType(), True), StructField('SITE_NAME', StringType(), True), StructField('COMID', DoubleType(), True), StructField('FEATUREID', LongType(), True), StructField('COUNTY', StringType(), True), StructField('STATE', StringType(), True), StructField('ECOREGION_I', LongType(), True), StructField('ECOREGION_II', DoubleType(), True), StructField('ECOREGION_III', StringType(), True), StructField('ECOREGION_IV', StringType(), True), StructField('ECONAME_I', StringType(), True), StructField('ECONAME_II', StringType(), True), StructField('ECONAME_III', StringType(), True), StructField('ECONAME_IV', StringType(), True), StructField('LAKE_DEPTH_MAX', DoubleType(), True), StructField('LAKE_DEPTH_MEAN', DoubleType(), True), StructField('LAKE_RET', DoubleType(), True), StructField('LAKE_AREA_HA', DoubleType(), True), StructField('LAKE_AREA_NHD2', DoubleType(), True), StructField('WSHD_AREA_HA', DoubleType(), True), StructField('SITE_ELEV', DoubleType(), True), StructField('WSHD_ELEV_AVG', DoubleType(), True), StructField('WSHD_ELEV_MIN', DoubleType(), True), StructField('WSHD_ELEV_MAX', DoubleType(), True), StructField('WSHD_SLOPE_AVG', DoubleType(), True), StructField('UPDATE_DATE', TimestampType(), True)])
2024-11-28 09:28:41,986 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-28 09:28:42,262 - DataLoader - INFO - Successfully wrote 175 records to /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-28 09:28:42,263 - DataLoader - INFO - Successfully loaded all static data
2024-11-28 09:28:42,411 - DataLoader - INFO - === Data Verification ===
2024-11-28 09:28:42,548 - DataLoader - INFO - Methods data count: 362
2024-11-28 09:28:42,549 - DataLoader - INFO - Methods columns: PROGRAM_ID, PARAMETER, START_YEAR, END_YEAR, METHOD, METHOD_DESCRIPTION
2024-11-28 09:28:42,626 - DataLoader - INFO - Site Information data count: 175
2024-11-28 09:28:42,627 - DataLoader - INFO - Site Information columns: SITE_ID, PROGRAM_ID, LATDD, LONDD, LATDD_CENTROID, LONDD_CENTROID, SITE_NAME, COMID, FEATUREID, COUNTY, STATE, ECOREGION_I, ECOREGION_II, ECOREGION_III, ECOREGION_IV, ECONAME_I, ECONAME_II, ECONAME_III, ECONAME_IV, LAKE_DEPTH_MAX, LAKE_DEPTH_MEAN, LAKE_RET, LAKE_AREA_HA, LAKE_AREA_NHD2, WSHD_AREA_HA, SITE_ELEV, WSHD_ELEV_AVG, WSHD_ELEV_MIN, WSHD_ELEV_MAX, WSHD_SLOPE_AVG, UPDATE_DATE
2024-11-28 09:28:42,627 - DataLoader - INFO - 
Sample of Methods data:
2024-11-28 09:28:43,194 - DataLoader - INFO - 
Sample of Site Information data:
2024-11-28 09:28:43,314 - py4j.clientserver - INFO - Closing down clientserver connection
2024-11-28 09:37:34,350 - DataLoader - INFO - Initialized Spark session
2024-11-28 09:37:34,350 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-28 09:37:34,438 - DataLoader - INFO - Inferred schema: StructType([StructField('PROGRAM_ID', StringType(), True), StructField('PARAMETER', StringType(), True), StructField('START_YEAR', LongType(), True), StructField('END_YEAR', DoubleType(), True), StructField('METHOD', StringType(), True), StructField('METHOD_DESCRIPTION', StringType(), True)])
2024-11-28 09:37:35,312 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 09:37:37,127 - DataLoader - INFO - Successfully wrote 362 records to /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 09:37:37,127 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-28 09:37:37,178 - DataLoader - INFO - Inferred schema: StructType([StructField('SITE_ID', StringType(), True), StructField('PROGRAM_ID', StringType(), True), StructField('LATDD', DoubleType(), True), StructField('LONDD', DoubleType(), True), StructField('LATDD_CENTROID', DoubleType(), True), StructField('LONDD_CENTROID', DoubleType(), True), StructField('SITE_NAME', StringType(), True), StructField('COMID', DoubleType(), True), StructField('FEATUREID', LongType(), True), StructField('COUNTY', StringType(), True), StructField('STATE', StringType(), True), StructField('ECOREGION_I', LongType(), True), StructField('ECOREGION_II', DoubleType(), True), StructField('ECOREGION_III', StringType(), True), StructField('ECOREGION_IV', StringType(), True), StructField('ECONAME_I', StringType(), True), StructField('ECONAME_II', StringType(), True), StructField('ECONAME_III', StringType(), True), StructField('ECONAME_IV', StringType(), True), StructField('LAKE_DEPTH_MAX', DoubleType(), True), StructField('LAKE_DEPTH_MEAN', DoubleType(), True), StructField('LAKE_RET', DoubleType(), True), StructField('LAKE_AREA_HA', DoubleType(), True), StructField('LAKE_AREA_NHD2', DoubleType(), True), StructField('WSHD_AREA_HA', DoubleType(), True), StructField('SITE_ELEV', DoubleType(), True), StructField('WSHD_ELEV_AVG', DoubleType(), True), StructField('WSHD_ELEV_MIN', DoubleType(), True), StructField('WSHD_ELEV_MAX', DoubleType(), True), StructField('WSHD_SLOPE_AVG', DoubleType(), True), StructField('UPDATE_DATE', TimestampType(), True)])
2024-11-28 09:37:37,237 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-28 09:37:37,536 - DataLoader - INFO - Successfully wrote 175 records to /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-28 09:37:37,536 - DataLoader - INFO - Successfully loaded all static data
2024-11-28 09:37:37,682 - DataLoader - INFO - === Data Verification ===
2024-11-28 09:37:37,829 - DataLoader - INFO - Methods data count: 362
2024-11-28 09:37:37,830 - DataLoader - INFO - Methods columns: PROGRAM_ID, PARAMETER, START_YEAR, END_YEAR, METHOD, METHOD_DESCRIPTION
2024-11-28 09:37:37,889 - DataLoader - INFO - Site Information data count: 175
2024-11-28 09:37:37,890 - DataLoader - INFO - Site Information columns: SITE_ID, PROGRAM_ID, LATDD, LONDD, LATDD_CENTROID, LONDD_CENTROID, SITE_NAME, COMID, FEATUREID, COUNTY, STATE, ECOREGION_I, ECOREGION_II, ECOREGION_III, ECOREGION_IV, ECONAME_I, ECONAME_II, ECONAME_III, ECONAME_IV, LAKE_DEPTH_MAX, LAKE_DEPTH_MEAN, LAKE_RET, LAKE_AREA_HA, LAKE_AREA_NHD2, WSHD_AREA_HA, SITE_ELEV, WSHD_ELEV_AVG, WSHD_ELEV_MIN, WSHD_ELEV_MAX, WSHD_SLOPE_AVG, UPDATE_DATE
2024-11-28 09:37:37,891 - DataLoader - INFO - 
Sample of Methods data:
2024-11-28 09:37:38,408 - DataLoader - INFO - 
Sample of Site Information data:
2024-11-28 09:37:38,540 - py4j.clientserver - INFO - Closing down clientserver connection
2024-11-28 10:27:24,978 - DataLoader - INFO - Initialized Spark session
2024-11-28 10:27:24,979 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Methods.xlsx
2024-11-28 10:27:25,095 - DataLoader - INFO - Inferred schema: StructType([StructField('PROGRAM_ID', StringType(), True), StructField('PARAMETER', StringType(), True), StructField('START_YEAR', LongType(), True), StructField('END_YEAR', DoubleType(), True), StructField('METHOD', StringType(), True), StructField('METHOD_DESCRIPTION', StringType(), True)])
2024-11-28 10:27:26,046 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 10:27:28,001 - DataLoader - INFO - Successfully wrote 362 records to /Users/work/Documents/pf-data-integration/data/processed/methods
2024-11-28 10:27:28,001 - DataLoader - INFO - Reading Excel file: /Users/work/Documents/pf-data-integration/data/raw/Site Information.xlsx
2024-11-28 10:27:28,054 - DataLoader - INFO - Inferred schema: StructType([StructField('SITE_ID', StringType(), True), StructField('PROGRAM_ID', StringType(), True), StructField('LATDD', DoubleType(), True), StructField('LONDD', DoubleType(), True), StructField('LATDD_CENTROID', DoubleType(), True), StructField('LONDD_CENTROID', DoubleType(), True), StructField('SITE_NAME', StringType(), True), StructField('COMID', DoubleType(), True), StructField('FEATUREID', LongType(), True), StructField('COUNTY', StringType(), True), StructField('STATE', StringType(), True), StructField('ECOREGION_I', LongType(), True), StructField('ECOREGION_II', DoubleType(), True), StructField('ECOREGION_III', StringType(), True), StructField('ECOREGION_IV', StringType(), True), StructField('ECONAME_I', StringType(), True), StructField('ECONAME_II', StringType(), True), StructField('ECONAME_III', StringType(), True), StructField('ECONAME_IV', StringType(), True), StructField('LAKE_DEPTH_MAX', DoubleType(), True), StructField('LAKE_DEPTH_MEAN', DoubleType(), True), StructField('LAKE_RET', DoubleType(), True), StructField('LAKE_AREA_HA', DoubleType(), True), StructField('LAKE_AREA_NHD2', DoubleType(), True), StructField('WSHD_AREA_HA', DoubleType(), True), StructField('SITE_ELEV', DoubleType(), True), StructField('WSHD_ELEV_AVG', DoubleType(), True), StructField('WSHD_ELEV_MIN', DoubleType(), True), StructField('WSHD_ELEV_MAX', DoubleType(), True), StructField('WSHD_SLOPE_AVG', DoubleType(), True), StructField('UPDATE_DATE', TimestampType(), True)])
2024-11-28 10:27:28,111 - DataLoader - INFO - Writing Parquet file to: /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-28 10:27:28,402 - DataLoader - INFO - Successfully wrote 175 records to /Users/work/Documents/pf-data-integration/data/processed/site_information
2024-11-28 10:27:28,402 - DataLoader - INFO - Successfully loaded all static data
2024-11-28 10:27:28,589 - DataLoader - INFO - === Data Verification ===
2024-11-28 10:27:28,744 - DataLoader - INFO - Methods data count: 362
2024-11-28 10:27:28,745 - DataLoader - INFO - Methods columns: PROGRAM_ID, PARAMETER, START_YEAR, END_YEAR, METHOD, METHOD_DESCRIPTION
2024-11-28 10:27:28,808 - DataLoader - INFO - Site Information data count: 175
2024-11-28 10:27:28,809 - DataLoader - INFO - Site Information columns: SITE_ID, PROGRAM_ID, LATDD, LONDD, LATDD_CENTROID, LONDD_CENTROID, SITE_NAME, COMID, FEATUREID, COUNTY, STATE, ECOREGION_I, ECOREGION_II, ECOREGION_III, ECOREGION_IV, ECONAME_I, ECONAME_II, ECONAME_III, ECONAME_IV, LAKE_DEPTH_MAX, LAKE_DEPTH_MEAN, LAKE_RET, LAKE_AREA_HA, LAKE_AREA_NHD2, WSHD_AREA_HA, SITE_ELEV, WSHD_ELEV_AVG, WSHD_ELEV_MIN, WSHD_ELEV_MAX, WSHD_SLOPE_AVG, UPDATE_DATE
2024-11-28 10:27:28,809 - DataLoader - INFO - 
Sample of Methods data:
2024-11-28 10:27:29,394 - DataLoader - INFO - 
Sample of Site Information data:
2024-11-28 10:27:29,531 - py4j.clientserver - INFO - Closing down clientserver connection
